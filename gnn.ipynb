{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from iou_graph import IOUGraph\n",
    "from dgl_reflacx_tools.tools import gridify, gridify_indices, gridify_by_indices, grid_readout\n",
    "\n",
    "from dgl_reflacx_tools.dgl_reflacx_collection import GraphCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pth = 'datasets/reflacx_densnet225_iou'\n",
    "collection = GraphCollection(dataset_pth, IOUGraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting sample batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "grid_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [collection.fetch_by_dgl_index(i) for i in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = [pair.dgl_graph for pair in pairs]\n",
    "labels = [pair.dgl_labels for pair in pairs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dgl.batch(graphs)\n",
    "labels = torch.cat(labels).reshape((batch_size, len(labels[0])))\n",
    "\n",
    "batch, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.ndata['h'] = torch.cat([batch.ndata['norm_x'].unsqueeze(1),\n",
    "                              batch.ndata['norm_y'].unsqueeze(1),\n",
    "                              batch.ndata['duration'].unsqueeze(1),\n",
    "                              batch.ndata['feats']],\n",
    "                             dim=1)\n",
    "batch.update_all(fn.copy_e('weight', 'm'), fn.sum('m', 'neigh_weight'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.ndata['h'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = batch.ndata['h'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.ndata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.edata.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convolution module on a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_messages(g, feat_nm, w_nm, sum_w_nm):\n",
    "    g.update_all(fn.v_mul_e(feat_nm, w_nm, 'm'), fn.sum('m', feat_nm))\n",
    "    g.ndata[feat_nm] = torch.divide(batch.ndata[feat_nm], batch.ndata[sum_w_nm].unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 grid_indices,\n",
    "                 pass_messages,\n",
    "                 activation=F.relu):\n",
    "        super(GridConv, self).__init__()\n",
    "        self.grid_lin = [[nn.Linear(in_feats, out_feats) for j in range(len(grid_indices[0]))]\n",
    "                         for i in range(len(grid_indices))]\n",
    "        self.grid_indices = grid_indices\n",
    "        self.pass_messages = pass_messages\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, graph, feat_nm, out_feat_nm=None):\n",
    "        # pass messages (convolution) in whole graph\n",
    "        self.pass_messages(graph, feat_nm)\n",
    "        \n",
    "        # activation on grid cell model\n",
    "        grid = gridify_by_indices(graph, self.grid_indices)\n",
    "        new_feats = None\n",
    "        i_s = None\n",
    "        for i, line in enumerate(grid):\n",
    "            for j, sg in enumerate(line):\n",
    "                conv_feats = self.activation(self.grid_lin[i][j](sg.ndata[feat_nm]))\n",
    "                \n",
    "                #concatenate new features to uptadate parent graph\n",
    "                if new_feats is None:\n",
    "                    new_feats = conv_feats\n",
    "                    i_s = self.grid_indices[i][j]\n",
    "                else:\n",
    "                    new_feats = torch.cat([new_feats, conv_feats])\n",
    "                    i_s = torch.cat([i_s, self.grid_indices[i][j]])\n",
    "        \n",
    "        #update parent graph with features calculated by grid\n",
    "        i_s = torch.sort(i_s).indices\n",
    "        new_feats = new_feats[i_s]\n",
    "        graph.ndata[feat_nm if out_feat_nm is None else out_feat_nm] = new_feats\n",
    "        \n",
    "        return new_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_message = lambda g, feat_nm: pass_messages(g, feat_nm, 'weight', 'neigh_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = GridConv(input_shape, input_shape, gridify_indices(batch, grid_size), f_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with batch.local_scope():\n",
    "   h = conv(batch, 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl_reflacx_tools.tools import Readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflacxReadout(Readout):\n",
    "    def __init__(self):\n",
    "        feats_and_aggrs = [('duration', lambda x, y: dgl.sum_nodes(x, y).cpu()),\n",
    "                           ('h', lambda x, y: dgl.mean_nodes(x, y).cpu())]\n",
    "        super().__init__(feats_and_aggrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 conv_dims,\n",
    "                 class_dims,\n",
    "                 readout_dim,\n",
    "                 n_classes,\n",
    "                 grid_indices,\n",
    "                 pass_messages,\n",
    "                 readout,\n",
    "                 conv_activation=F.relu,\n",
    "                 mlp_activation=F.relu): # TODO make possibel to have diff activations for conv and fc\n",
    "        super(Classifier, self).__init__()\n",
    "        self.grid_indices = grid_indices\n",
    "        new_conv = lambda in_dims, out_dims: GridConv(in_dims,\n",
    "                                                      out_dims,\n",
    "                                                      grid_indices,\n",
    "                                                      pass_messages,\n",
    "                                                      conv_activation)\n",
    "        self.convs = [new_conv(input_dim,\n",
    "                               (conv_dims[0]\n",
    "                                if len(conv_dims) > 0\n",
    "                                else input_dim))]\n",
    "        for i, dim in enumerate(conv_dims[1:], start=1):\n",
    "            self.convs.append(new_conv(conv_dims[i - 1], dim))\n",
    "\n",
    "        self.fcs = [nn.Linear(readout_dim if len(conv_dims) > 0 else input_dim,\n",
    "                              class_dims[0] if len(class_dims) > 0 else n_classes)]\n",
    "        for i, dim in enumerate(class_dims[1:], start=1):\n",
    "            self.fcs.append(nn.Linear(class_dims[i - 1], dim))\n",
    "        if len(self.fcs) > 1:\n",
    "            self.fcs.append(nn.Linear(class_dims[-1], n_classes))\n",
    "\n",
    "        self.readout = readout\n",
    "        self.conv_activation = conv_activation\n",
    "        self.mlp_activation = mlp_activation\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, graph, conv_feat_nm):\n",
    "        h = None\n",
    "        with graph.local_scope():\n",
    "            for conv_l in self.convs:\n",
    "                h = conv_l(graph, conv_feat_nm)\n",
    "                print('h', h.shape)\n",
    "            grid = gridify_by_indices(graph, self.grid_indices)\n",
    "            h = self.readout(grid)\n",
    "        print('ro', h.shape)\n",
    "        for fc_l in self.fcs[:-1]:\n",
    "            h = self.mlp_activation(fc_l(h))\n",
    "            print('h', h.shape)\n",
    "        h = self.fcs[-1](h)\n",
    "        print('preds', h.shape)\n",
    "\n",
    "        return h\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid_conv_seq(shapes,\n",
    "                         activation,\n",
    "                         grid_indices,\n",
    "                         pass_messages):\n",
    "    result = []\n",
    "    for i in range(len(shapes) - 1):\n",
    "        in_dim = shapes[i]\n",
    "        out_dim = shapes[i + 1]\n",
    "        result.append(('conv{}'.format(i + 1),\n",
    "                       GridConv(in_dim,\n",
    "                                out_dim,\n",
    "                                grid_indices,\n",
    "                                pass_messages,\n",
    "                                activation)))\n",
    "    return OrderedDict(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv1): GridConv()\n",
       "  (conv2): GridConv()\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Sequential(create_grid_conv_seq([input_shape, 200, 100], F.relu, gridify_indices(batch, grid_size), pass_messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 conv_dims,\n",
    "                 class_dims,\n",
    "                 readout_dim,\n",
    "                 n_classes,\n",
    "                 grid_indices,\n",
    "                 pass_messages,\n",
    "                 readout,\n",
    "                 conv_activation=F.relu,\n",
    "                 mlp_activation=F.relu): # TODO make possibel to have diff activations for conv and fc\n",
    "        super(Classifier2, self).__init__()\n",
    "        self.grid_indices = grid_indices\n",
    "        self.convs = nn.Sequential(create_grid_conv_seq([input_dim] + conv_dims,\n",
    "                                                        conv_activation,\n",
    "                                                        grid_indices,\n",
    "                                                        pass_messages))\n",
    "        \n",
    "        fsshapes = [readout_dim] + class_dims\n",
    "        fclist = []\n",
    "        for i in range(len(fsshapes) - 1):\n",
    "            fclist.append(('fc{}'.format(i + 1),\n",
    "                           nn.Linear(fsshapes[i], fsshapes[i + 1])))\n",
    "            fclist.append(('fc_activ{}'.format(i + 1),\n",
    "                           mlp_activation()))\n",
    "        self.fcs = nn.Sequential(OrderedDict(fclist))\n",
    "        \n",
    "        self.readout = readout\n",
    "        self.conv_activation = conv_activation\n",
    "        self.mlp_activation = mlp_activation\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, graph, conv_feat_nm):\n",
    "        h = None\n",
    "        with graph.local_scope():\n",
    "            h = self.convs(batch, 'h')\n",
    "            grid = gridify_by_indices(graph, self.grid_indices)\n",
    "            h = self.readout(grid)\n",
    "        h = self.fcs(h)\n",
    "        for fc_l in self.fcs[:-1]:\n",
    "        print('preds', h.shape)\n",
    "\n",
    "        return h\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier(input_shape,\n",
    "                 [100, 200],\n",
    "                 [50, 40],\n",
    "                 3216,\n",
    "                 6,\n",
    "                 gridify_indices(batch, grid_size),\n",
    "                 f_message,\n",
    "                 ReflacxReadout())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = Classifier2(input_shape,\n",
    "                 [100, 200],\n",
    "                 [50, 40],\n",
    "                 3216,\n",
    "                 6,\n",
    "                 gridify_indices(batch, grid_size),\n",
    "                 f_message,\n",
    "                 ReflacxReadout())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(clf.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = clf(batch, 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(clf.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m opt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m h \u001b[38;5;241m=\u001b[39m clf(batch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(h, labels) \u001b[38;5;66;03m# TODO check if this is the correct loss for regression\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/pyvenvs/graph_classification/lib/python3.8/site-packages/torch/optim/adam.py:45\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m     42\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m     43\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m     44\u001b[0m                 differentiable\u001b[38;5;241m=\u001b[39mdifferentiable, fused\u001b[38;5;241m=\u001b[39mfused)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[0;32m~/dev/pyvenvs/graph_classification/lib/python3.8/site-packages/torch/optim/optimizer.py:273\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    271\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    275\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "\n",
    "opt = torch.optim.Adam(clf.parameters())\n",
    "h = clf(batch, 'h')\n",
    "loss = F.cross_entropy(h, labels) # TODO check if this is the correct loss for regression\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "opt.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
