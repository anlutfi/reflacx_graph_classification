{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representando Dados do REFLACX como grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detalhamento do REFLACX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A figura abaixo ilustra a estrutura do REFLACX. Cada exame de imagem da base original (MIMIC-CXR) pode gerar várias observações, cada uma gerando um datapoint no REFLACX.\n",
    "Cada ponto do REFLACX contem 5 atributos relevantes, em verde na imagem. São eles: Uma série de rótulos binários para descrever a presença de anomalias; o caminho do olhar do médico; a transcrição da voz do médico durante o processo; as coordenadas do retângulo que contem o tórax na radiografia original; e as coordenadas das elipses que contém as anomalias presentes.\n",
    "\n",
    "O trabalho de enriquecimento de dados realizado até o momento --em azul-- foi o de dividir a transcrição em frases e, analogamente, dividir as fixações do olhar para cada uma destas frases.\n",
    "\n",
    "![](readme_files/reflacx_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O próximo passo nesta pesquisa é tratar cada um desses grupos de fixações por frase como um grafo, para que o diagnóstico das anomalias seja aprendido por uma Graph Convolutional Network(GCN).\n",
    "\n",
    "Para facilitar o entendimento, segue uma descrição em mais detalhes a estrutura de cada fixação do olhar na base do REFLACX:\n",
    "1. timestamps de início e final da fixação\n",
    "2. posição (x,y) da fixação no espaço da radiografia inteira, e não da tela\n",
    "3. área medida da pupila do radiologista\n",
    "4. resoluções angulares, vertical e horizontal, da radiografia para o nível de zoom expecífico da fixação. Ou seja, quantos pixels são observados por grau do cone de visão do radiologista\n",
    "5. métricas da janela do software de observação\n",
    "6. as coordenadas do crop do exame exibido na tela no espaço da imagem inteira\n",
    "7. a posição deste crop em coordenadas da tela\n",
    "\n",
    "Para a modelagem desta estrutura de dados, descrita mais adiante, foi necessário considerar o que seria importante para um vértice, bem como nos critérios para que dois vértices sejam considerados vizinhos, partilhando uma aresta.\n",
    "\n",
    "### Estrutura de Grafo Proposta\n",
    "\n",
    "#### Vértices\n",
    "\n",
    "Para o vértice em si, 3 grandezas foram consideradas relevantes: o tempo que o radiologista repousou o olhar; a posição daquela observação em relação ao tórax; e a região da imagem que se observou naquela fixação. O tempo se obtem facilmente pelo atributo 1 descrito acima. A posição relativa ao tórax pode ser obtida transformando a posição do espaço da imagem para o espaço da bounding box do torax, normalizando-a para [0,1]. Já a região da imagem considerada observada é importante para feature extraction. Seguindo o método do próprio código do REFLACX para a geração de heatmaps, a região observada em uma determinada fixação é representada por uma distribuição normal com média igual à posição da fixação e desvio padrão igual resolução angular (item 4 acima). Sendo assim, o recorte da radiografia a ser usado para representar uma fixação pode ser definido por N = número de desvios padrões que se deseja considerar.\n",
    "\n",
    "![](readme_files/nodes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "O modelo utilizado como feature extractor é o densenet121-res224-mimic_ch (https://github.com/mlmed/torchxrayvision, Cohen2022xrv, chexpert: irvin2019chexpert). A última camada de features, antes da planificação do tensor e sua subsequente classificação, tem dimensões 7 X 7 X 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_meta_path = '../reflacx_lib/full_meta.json' # if file doesn't exist, it will be created\n",
    "reflacx_dir = \"../data/reflacx\"\n",
    "mimic_dir = \"../data/mimic/reflacx_imgs\"\n",
    "\n",
    "from metadata import Metadata\n",
    "\n",
    "metadata = Metadata(reflacx_dir, mimic_dir, full_meta_path, max_dicom_lib_ram_percent=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_id = '0658ad3c-b4f77a56-2ed1609f-ea71a443-d847a975'\n",
    "reflacx_id = 'P109R167865'\n",
    "sample = metadata.get_sample(dicom_id, reflacx_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_extraction.dense_feature_extraction import DenseFeatureExtractor\n",
    "extractor = DenseFeatureExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalização pela média\n",
    "\n",
    "Dado que o DensNet não subtrai a imagem de entrada da média das imagens da base, uma normalização pela media pode ser feita já no espaço das features.\n",
    "Calculando a média das features 7x7x1024 de cada imagem do REFLACX, e usando sua subtração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_fpath = './avg_DensNet_REFLACX_features.npy'\n",
    "\n",
    "try:\n",
    "    avg_feats = np.load(avg_fpath)\n",
    "except FileNotFoundError:\n",
    "    avg_feats = extractor.get_reflacx_avg_features(metadata, True)\n",
    "    np.save(avg_fpath, avg_feats)\n",
    "\n",
    "avg_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_feats[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extração propriamente dita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extractor.get_reflacx_img_features(sample)\n",
    "norm_features = extractor.get_reflacx_img_features(sample, mean_features=torch.from_numpy(avg_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape, avg_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features[0][0,:], norm_features[0][0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction para cada Fixation\n",
    "\n",
    "As features acima representam a imagem inteira. Mais especificamente, cada um dos 7 X 7 tensores divide a imagem em 49 regiões e descreve cada uma delas. Existem 1024 destes tensores 7X7 para cada imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = features[0].tolist()\n",
    "for line in example:\n",
    "    for n in line:\n",
    "        print(\"{:.3f} \".format(n), end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iou_graph import IOUGraph\n",
    "from scanpath_graph import ScanPathGraph\n",
    "from euclidean_graph import EuclideanGraph\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minf = None\n",
    "mind = None\n",
    "minr = None\n",
    "for did in metadata.list_dicom_ids():\n",
    "    for rid in metadata.list_reflacx_ids(did):\n",
    "        sample = metadata.get_sample(did, rid)\n",
    "        l = len(sample.get_fixations())\n",
    "        if l != 0 and (minf is None or l < minf):\n",
    "            mind = did\n",
    "            minr = rid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = metadata.get_sample(mind, minr)\n",
    "len(sample.get_fixations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mind, minr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = IOUGraph(did,\n",
    "             rid,\n",
    "             sample,\n",
    "             metadata=metadata,\n",
    "             stdevs=1,\n",
    "             feature_extractor=extractor,\n",
    "             img_features=extractor.get_reflacx_img_features(sample).detach().numpy(),\n",
    "             mean_features=avg_feats)\n",
    "\n",
    "gsc = ScanPathGraph(did,\n",
    "                    rid,\n",
    "                    sample,\n",
    "                    metadata=metadata,\n",
    "                    stdevs=1,\n",
    "                    feature_extractor=extractor,\n",
    "                    img_features=extractor.get_reflacx_img_features(sample).detach().numpy(),\n",
    "                    mean_features=avg_feats)\n",
    "\n",
    "ge = EuclideanGraph(did,\n",
    "                    rid,\n",
    "                    sample,\n",
    "                    metadata=metadata,\n",
    "                    stdevs=1,\n",
    "                    feature_extractor=extractor,\n",
    "                    img_features=extractor.get_reflacx_img_features(sample).detach().numpy(),\n",
    "                    mean_features=avg_feats)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.draw('.', 'teste.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = [51, 39]\n",
    "sz = 500\n",
    "canvas = np.zeros((sz, sz, 3))\n",
    "umask = np.zeros((sz, sz))\n",
    "imask = np.zeros((sz, sz))\n",
    "first = True\n",
    "\n",
    "for i, node in enumerate(g.nodes):\n",
    "    if len(idxs) > 0 and i not in idxs:\n",
    "        continue\n",
    "    tlx, tly = node.topleft\n",
    "    brx, bry = node.bottomright\n",
    "    x = node.norm_x\n",
    "    y = node.norm_y\n",
    "    tlx = int(tlx * sz)\n",
    "    tly = int(tly * sz)\n",
    "    brx = int(brx * sz)\n",
    "    bry = int(bry * sz)\n",
    "    x = int(x * sz)\n",
    "    y = int(y * sz)\n",
    "\n",
    "    canvas = cv2.circle(canvas, (x, y), 3, (255, 0, 0), -1)\n",
    "    canvas = cv2.rectangle(canvas, (tlx, tly), (brx, bry), (0, 255, 0), 2)\n",
    "    if first:\n",
    "        first = False\n",
    "        imask = cv2.rectangle(imask, (tlx, tly), (brx, bry), 255, -1)\n",
    "        umask = cv2.rectangle(umask, (tlx, tly), (brx, bry), 255, -1)\n",
    "\n",
    "        imask = cv2.threshold(imask, 200, 255, cv2.THRESH_BINARY)[1]\n",
    "        umask = cv2.threshold(umask, 200, 255, cv2.THRESH_BINARY)[1]\n",
    "        \n",
    "\n",
    "    else:\n",
    "        m = np.zeros((sz, sz))\n",
    "        m = cv2.rectangle(m, (tlx, tly), (brx, bry), 255, -1)\n",
    "        m = cv2.threshold(m, 200, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "        imask[imask != m] = 0\n",
    "        umask[umask != m] = 255  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(umask, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imask, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union = np.count_nonzero(umask)\n",
    "inter = np.count_nonzero(imask)\n",
    "\n",
    "union, inter, inter / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0,0,0,1,1])\n",
    "y = np.array([1,0,0,1,0])\n",
    "\n",
    "x & y, x | y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas[canvas == 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = metadata.get_sample('0658ad3c-b4f77a56-2ed1609f-ea71a443-d847a975', 'P109R167865')\n",
    "g = IOUGraph('0658ad3c-b4f77a56-2ed1609f-ea71a443-d847a975',\n",
    "             'P109R167865',\n",
    "             sample,\n",
    "             metadata=metadata,\n",
    "             stdevs=1,\n",
    "             feature_extractor=extractor,\n",
    "             img_features=extractor.get_reflacx_img_features(sample).detach().numpy(),\n",
    "             mean_features=avg_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_i = g.nodes[idxs[0]]\n",
    "node_j = g.nodes[idxs[1]]\n",
    "\n",
    "i_x_min, i_y_min = node_i.topleft\n",
    "i_x_max, i_y_max = node_i.bottomright\n",
    "j_x_min, j_y_min = node_j.topleft\n",
    "j_x_max, j_y_max = node_j.bottomright\n",
    "\n",
    "#i_x_min = int(i_x_min * sz)\n",
    "#i_y_min = int(i_y_min * sz)\n",
    "#i_x_max = int(i_x_max * sz)\n",
    "#i_y_max = int(i_y_max * sz)\n",
    "#j_x_min = int(j_x_min * sz)\n",
    "#j_y_min = int(j_y_min * sz)\n",
    "#j_x_max = int(j_x_max * sz)\n",
    "#j_y_max = int(j_y_max * sz)\n",
    "\n",
    "xA = max(i_x_min, j_x_min)\n",
    "yA = max(i_y_min, j_y_min)\n",
    "xB = min(i_x_max, j_x_max)\n",
    "yB = min(i_y_max, j_y_max)\n",
    "\n",
    "intersec = max(0, xB - xA) * max(0, yB - yA)\n",
    "\n",
    "i_area = ((i_x_max - i_x_min) * (i_y_max - i_y_min))\n",
    "j_area = ((j_x_max - j_x_min) * (j_y_max - j_y_min))\n",
    "\n",
    "#intersec = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "#\n",
    "#i_area = ((i_x_max - i_x_min + 1) * (i_y_max - i_y_min + 1))\n",
    "#j_area = ((j_x_max - j_x_min + 1) * (j_y_max - j_y_min + 1))\n",
    "\n",
    "iou = intersec / (i_area + j_area - intersec)\n",
    "\n",
    "(i_area + j_area - intersec), intersec, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.edges['edges'][idxs[1], idxs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](feature_grid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se torna necessário determinar, para cada fixation e sua respectiva região de atenção, quais regiões considerar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](feature_grid_fixation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O crop da fixation está localizado em mais de uma região (quanto mais distante for o zoom, mais regiões). Para determinar um array de features para este ponto, faz-se uma média das features de cada região que o compreende, ponderada pelas áreas correspondentes. O resultado é um tensor de 1024 X 1, do mesmo formato das features extraídas da DensNet, porém só considerando as regiões relevantes para uma fixation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixations_features = extractor.get_all_fixations_features(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixations_features[10].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arestas\n",
    "\n",
    "Já para as arestas, outras 3 medidas foram consideradas até o momento:\n",
    "1. Gaze Path Edges: a posição de uma fixação na sequência de fixações do caminho do olhar. Uma fixação é vizinha da anterior e da próxima\n",
    "2. Euclidean Edges: a distância euclideana entre duas observações, com peso da aresta maior para vértices mais próximos. Como cada vértice possui sua posição (x, y) normalizada para [0, 1], o peso da aresta é 2^0.5 - distância.\n",
    "3. IOU edges: a similaridade entre as regiões da imagem obtidas entre duas fixações. Dois vértices podem estar próximos em distância, mas, devido a possíveis diferenças do nível de zoom a cada fixação, não possuirem muita área da imagem em comum. O peso da aresta é dado pela IOU das imagens de ambos os vértices (talvez seja interessante linearizar isso e usar sqrt(IOU))\n",
    "\n",
    "![](readme_files/edges.png)\n",
    "\n",
    "Pode-se fazer um dataset para cada tipo de aresta, ou um único dataset, com grafos heterogêneos. É necessário cuidado neste caso, pois IOU e distâncias euclideanas possuem correlação. No momento, o treinamento inicial está sendo formulado com quatro grafos: um para cada tipo de aresta descrita acima, e um que combina arestas de IOU e Gaze Path.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolução em uma GNN: Message Passing\n",
    "\n",
    "Em uma rede em grafo, a convolução é uma operação que atualiza certas features de cada nó, agregando informações dos vizinhos. Um exemplo no caso do grafo proposto aqui, seria uma média das features extraídas para cada vértice, ponderada pelo tempo de permanência do olhar (duration). Depois de algumas camadas de convolução, o grafo é resumido usando alguma função agregadora dos vértices, como média, max, etc. Para daí, então, o grafo ser classificado como um todo.\n",
    "\n",
    "No caso específico do REFLACX, duas questões surgem por estas agregações, tanto no nível do nó a cada convolução, quanto do grafo como um todo no final. Pelo fato das imagens serem todas radiografias de tórax alinhadas da mesma forma, as features extraídas para cada fixation não podem ser comparadas entre si com uma simples média ou soma. Features importantes na região cardíaca não necessáriamente são importantes na área do diafragma e vice-versa, por exemplo.\n",
    "\n",
    "Por esta razão, faz-se necessário abordar este problema preservando as particularidades espaciais de cada nó, ou, pelo menos, de cada região.\n",
    "\n",
    "A solução proposta neste momento para isso é usar as arestas de IOU para a convolução, de forma que cada nó fique restrito a receber informações com mais peso de sua vizinhança imediata. No Final de todas as camadas de convolução, pode-se dizer que cada componente conexo (rever a figura acima, extrema direita) representa uma subregião relevante da imagem, que pode ser representada, com menos perda, por uma função agregadora.\n",
    "\n",
    "Essa abordagem geraria um novo grafo com \"super-nós\". Deste ponto em diante, alguma operação que preserve a espacialidade destas regiões teria que ser realizada, antes da classificação final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preocupações Antecipadas\n",
    "\n",
    "A estrutura proposta sugere alguns pontos de preocupação que devem ser estudados durante sua implementação\n",
    "\n",
    "#### Orientação das arestas de Gaze Path\n",
    "Embora a abordagem mais simples para este tipo de aresta seja considerá-la não direcionada, existe um ponto a favor de um direcionamento. Uma fixação no instante t esclarece uma questão levantada em instantes ateriores, bem como levanta questões que serão esclarecidas em instantes futuros. Portanto, pode ser que seja importante diferenciar a \"ida\" da \"volta\", ou seja: talvez o peso da aresta que levanta questões seja diferente do peso da aresta que às esclarece."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_viz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
